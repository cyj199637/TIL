# 07. Network

## OSI 7계층?

1. Physical Layer
   * 0과 1의 나열을 아날로그 신호로 바꾸어 전선으로 흘려 보내고, 아날로그 신호가 들어오면 0과 1의 나열로 해석하여 물리적으로 연결된 두 컴퓨터가 0과 1의 나열을 주고받을 수 있게 해주는 모듈
   * 하드웨어적으로 구현되어 있음
2. Data-link Layer
   * 같은 네트워크(하나의 스위치로 연결된 컴퓨터들)에 있는 여러 대의 컴퓨터들이 데이터를 주고받기 위해서 필요한 모듈
   * 하드웨어적으로 구현되어 있음
3. Network Layer
   * 수많은 네트워크로 연결로 이루어지는 인터넷에서 어딘가에 있는 목적지 컴퓨터로 데이터를 전송하기 위해 IP 주소를 이용해 길을 찾고(Routing), 자신 다음의 라우터에게 데이터를 넘겨주는 것(Forwarding)
   * 운영체제 커널에 소프트웨어적으로 구현되어 있음
4. Transport Layer
   * 포트 번호를 사용하여 도착지 컴퓨터의 최종 도착지인 프로세스(실행 중인 프로그램)까지 데이터가 도달하게 하는 모듈
   * 운영체제 커널에 소프트웨어적으로 구현되어 있음
5. Application Layer
   * 기존의 OSI 모델의 Session Layer, Presentation Layer, Application Layer가 Application Layer가 합쳐짐



## HTTP vs HTTPS?

https://jeong-pro.tistory.com/89?category=793347



## GET vs POST?

### GET

* 쿼리 문자열을 사용하여 URI와 함께 요청 파라미터가 전송됨
  * 전송할 수 있는 파라미터 값의 길이에 제한이 있을 수 있음



### POST

* 데이터 영역을 이용해서 요청 파라미터를 전송
  * 전송할 수 있는 파라미터의 길이에 제한이 없음



## Session vs Cookie?

* 쿠키는 웹 브라우저(클라이언트)에 값을 저장하는 반면, 세션은 서버에 저장한다.

  → 쿠키는 네트워크를 통해 전달되기 때문에 HTTP 프로토콜을 사용하는 경우 중간에 누군가가 쿠키의 값을 읽어올 수 있다. 하지만, 세션은 서버에만 저장되기 때문에 상대적으로 쿠키보다는 세션이 보안성이 좋다.

* 웹 브라우저가 쿠키를 지원하지 않거나 강제로 쿠키를 막으면 쿠키는 사용할 수 없지만, 세션은 쿠키 설정 여부에 상관없이 사용할 수 있다.

* 쿠키는 도메인을 이용해서 쿠키를 여러 도메인 주소에 공유할 수 있지만, 세션은 여러 서버에서 공유할 수 없다.



## 캐시?

데이터나 값을 미리 복사해 놓는 임시 저장소

### 캐시를 사용하는 이유

* 원본 데이터에 접근하는 데 오래 걸리는 경우
* 값을 다시 계산하는 시간(비용)을 줄이고 싶은 경우

### HTTP 캐시

1. 서버가 응답 결과로 여러 리소스를 보내면 클라이언트(웹 브라우저)는 브라우저 캐시에 저장한다.
2. 클라이언트는 서버로 요청을 보내기 전에 브라우저 캐시부터 확인한다.
3. 캐시에 원하는 데이터가 있다면 해당 데이터를 사용한다.

만약 서버에서 데이터가 변경이 되었다면 클라이언트도 서버에서 데이터를 가져와 캐시 데이터를 갱신해야 한다. 하지만 클라이언트는 캐시 데이터가 삭제되지 않는 이상 계속해서 캐시 데이터를 사용한다.

→ 서버는 리소스를 응답할 때 `Cache-Control` 헤더의 `max-age` 에 해당 리소스의 만료 시간을 명시

그러나 만료 시간이 지나더라도 캐시 업데이트가 필요하지 않을 수 있다. 이 경우 캐시의 만료 시간이 지날 때마다 똑같은 리소스를 서버에서 다운로드하는 불필요한 네트워크 비용이 발생할 수 있다.

1. 서버에서 리소스를 응답할 때 `Etag` 헤더에 해시 값을 담아 보냄
2. 클라이언트는 해당 리소스가 만료되면 `If-None-Match` 헤더에 1에서 받은 해시 값을 담아 보냄
3. 만약 리소스가 변경되지 않았다면 해시 값이 같으므로 리소스를 새로 보내지 않고 `304 Not-Modified` 상태 코드와 함께 `Etag` 값을 보냄
4. 클라이언트는 기존에 가지고 있던 캐시 데이터를 재사용하게 됨

* Etag: 리소스의 컨텐츠에 해싱 알고리즘을 적용하여 얻은 데이터의 해시 값.

브라우저에 캐싱된 리소스는 만료될 때까지 사용해야 한다는 한계가 있다.

→ 리소스 URL을 변경하여 해결

* 파일의 디지털 지문이나 버전 번호를 파일 이름에 포함하는 방식으로 수행



## 웹 소캣?

두 프로그램 간의 메시지를 교환하기 위한 통신 방법 중 하나

<특징>

1. 양방향 통신: 데이터 송수신을 동시에 처리하기 위한 통신 방법
   * 클라이언트와 서버가 서로가 서로에게 원할 때 데이터를 주고 받을 수 있다.
2. 실시간 네트워킹: 웹 환경에서 연속된 데이터를 빠르게 노출
   * ex) 채팅, 주식, 비디오 데이터
   * 여러 단말기에서 빠르게 데이터를 교환

<장점>

<한계>



## 소켓 통신(TCP vs UDP)?



## RESTful API?

https://jeong-pro.tistory.com/168?category=793347

REST란 자원의 표현(HTTP URI)을 가지고 상태를 전달(HTTP Method)하는 것이다. RESTful이란 REST 아키텍처 스타일의 제약조건을 모두 만족하는 시스템이다.

<REST 아키텍처의 제약조건>

1. Client - Server
2. Stateless
3. Cache
4. Uniform Interface
5. Layered System
6. Code-On-Demand(Option)



## 웹 서버 vs WAS?

https://jeong-pro.tistory.com/84?category=793347

![WAS](../images/WAS.png)

* 웹 서버: 클라이언트로부터 HTTP 요청을 받아 HTML 문서나 각종 리소스를 전달하는 컴퓨터 ex) Apache, Nginx

  * 클라이언트의 요청을 기다리고 요청에 대한 데이터를 만들어서 응답
  * 응답 데이터는 정적인 데이터(html, css, 이미지 등)으로 한정
  * 사용자(요청)에 따라 다른 처리를 해줄 수 없음

* 웹 어플리케이션 서버(WAS): 웹 어플리케이션과 서버 환경을 만들어 동작시키는 기능을 제공하는 소프트웨어

  ​											   프레임워크(= Web Server + Web Container) ex) Tomcat

  * 웹 어플리케이션: 웹 브라우저에서 이용할 수 있는 응용 소프트웨어
  * 정적인 페이지에서 처리할 수 없는 비즈니스 로직이나 DB 조회 같은 동적인 컨텐츠 제공

### 그런데도 대규모 프로젝트에서는 웹 서버와 WAS를 따로 두는 이유?

![MultipleWAS](../images/MultipleWAS.png)

* 기능을 분리하여 서버 부하 방지

  : WAS는 DB 조회 등 페이지를 만들기 위한 다양한 로직을 처리하는데 정적 컨텐츠까지 WAS에서 처리한다면 다른 작업에 사용할 리소스들로 인해 지연 발생 가능 (Tomcat 5.5 이상부터는 성능이 크게 떨어지지 않음)

* 물리적으로 분리하여 보안 강화

  * SSL에 대한 암복호화 처리에 웹 서버 사용
  * 공격에 대해 웹 서버를 앞단에 두어 중요한 정보가 담긴 DB나 로직까지 전파되지 않도록 하기 위함

* 여러 대의 WAS를 연결 가능

  * Load Balancing
  * 서버를 여러 대 사용하는 대용량 웹 어플리케이션의 경우 웹 서버와 WAS를 분리하여 무중단 운영을 위한 장애 극복에 쉽게 대응할 수 있음
    * Fail Over: 하나의 WAS가 장애로 인해 작동이 중지되더라도 나머지 WAS들로 어플리케이션을 운영하는 것
    * Fail Back: 작동이 중지된 WAS를 다시 재가동 시키는 것
* 다른 종류의 WAS로 서비스 가능



## 프록시 서버?

클라이언트와 서버간의 중계 서버로, 통신을 대리 수행하는 서버

* 캐시 / 보안 / 트래픽 분산 등 여러 장점을 가질 수 있음
* 포워드 프록시, 리버스 프록시, 로드 밸런싱 등의 역할을 함

### 포워드 프록시?

: 클라이언트와 인터넷 사이에 위치하여 통신을 대리 수행하는 것

* 캐싱: 클라이언트가 요청한 내용을 캐싱(클라이언트가 누구냐와 상관없이 같은 내용을 요청하면 돌려줌)
  * 전송 시간 절약
  * 불필요한 외부 전송 X
  * 외부 요청 감소 → 네트워크 병목 현상 방지
* 익명성: 클라이언트가 보낸 요청을 감춤
  * 서버 입장에서는 클라이언트가 아니라 마치 포워드 프록시가 요청한 것처럼 보이므로 요청을 누가 보냈는지 알 수 없게 됨 (서버가 받은 요청 IP = Proxy IP)

### 리버스 프록시?

: 인터넷과 서버 사이에 위치하여 통신을 대리 수행하는 것

* 캐싱: 클라이언트가 요청한 내용을 캐싱

* 보안: 서버 정보를 클라이언트로부터 숨김

  * 클라이언트 입장에서는 리버스 프록시가 실제 서버라고 생각하고 요청을 전달함

    (실제 서버는 알지 못함 → 실제 서버의 IP가 노출되지 않음)

  * 요청을 받은 리버스 프록시는 자신이 알고 있는 실제 서버에 요청을 전달

* 로드 밸런싱



## 로드밸런서?

https://deveric.tistory.com/91

여러 대의 서버가 분산 처리할 수 있도록 요청을 나누어 주는 서비스

* OSI 7 계층을 기준으로 어떤 것을 나누는지에 따라 다름
  * L2: Mac 주소를 바탕으로 로드 밸런싱
  * L3: IP 주소를 바탕으로 로드 밸런싱
  * L4: Transport Layer(IP & Port) 레벨에서 로드 밸런싱
    * ex) https://www.youtube.com/로 접근 시, 서버 A와 서버 B로 로드 밸런싱
  * L7: Application Layer(User Request) 레벨에서 로드 밸런싱
    * ex) https://www.youtube.com/로 접근 시, /category 와 /search를 담당 서버들로 로드 밸런싱



## NGINX?

* **UpStream Module**: NGINX가 부하분산, 속도 개선과 같은 역할을 할 수 있게 해주는 내장 모듈
  * 업스트림 모듈로 한 대의 웹서버에 여러 대의 애플리케이션 서버를 붙임
  * 이후 클라이언트의 요청이 오면 여러 서버 중 요청을 처리 할 서버를 선정하여 요청을 전달하고, 서버의 응답도 클라이언트에게 전달 
  * **UpStream**: 여러 대의 컴퓨터가 순차적으로 어떤 일을 처리 할 때 어떤 서비스를 받는 서버 = Origin 서버

### restart vs reload

* restart: 서버를 shutdown한 후 재기동 → 서버가 운영되지 않는 시간이 존재
  * 설정 파일에 문법적 에러가 존재하는 경우, 서버가 죽음 
* reload: 새로운 설정 파일을 적용해도 서버가 shutdown되지 않음
  * 마스터 프로세스는 설정을 다시 로드하라는 신호를 받으면 새 설정 파일의 `syntax validity` 를 확인하고 제공된 설정한다. 성공하면 마스터 프로세스는 새 작업 프로세스를 시작하고 기존 작업 프로세스에 메시지를 보내 종료하도록 요청한다. 설정 파일에 문법적 에러가 발견되면 마스터 프로세스가 변경 사항을 롤백하고 이전 설정으로 계속 작업한다. 기존 작업 프로세스는 종료 명령을 받으면 새로운 요청은 더 이상 받지 않고 남은 요청이 모두 처리 될 때까지 계속 작업한다. 그 후 이전 작업자 프로세스가 종료된다.



## google.com을 주소창에 치고 화면에 구글 화면에 나올 때까지 어떤 일이 일어나는지 자세히 말해보시오.

https://deveric.tistory.com/97

: 3 way handshaking / 방화벽 / 로드밸런싱 / tomcat / 서버 쪽으로 넘어가서 요청을 어떻게 처리하는 것까지 자세하게 http/handshaking/방화벽/네트워크/쿼리파싱 답변에 추가